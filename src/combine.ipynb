{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71cf289",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuration \n",
    "'''Project globals'''\n",
    "\n",
    "# Paths\n",
    "WORKING_DIRECTORY='..'\n",
    "DATA_DIRECTORY=f'{WORKING_DIRECTORY}/data'\n",
    "RAW_DATA_DIRECTORY=f'{DATA_DIRECTORY}/raw'\n",
    "INTERIM_DATA_DIRECTORY=f'{DATA_DIRECTORY}/interim'\n",
    "PROCESSED_DATA_DIRECTORY=f'{DATA_DIRECTORY}/processed'\n",
    "MODEL_DIRECTORY=f'{WORKING_DIRECTORY}/models'\n",
    "\n",
    "# Data files\n",
    "RAW_INCIDENTS_MDB_FILE=f'{RAW_DATA_DIRECTORY}/avall.mdb'\n",
    "RAW_INCIDENTS_CSV_FILE=f'{RAW_DATA_DIRECTORY}/incidents.csv'\n",
    "EXTRACTED_INCIDENTS_FILE=f'{INTERIM_DATA_DIRECTORY}/incidents.csv'\n",
    "RAW_ONTIME_CSV_FILE=f'{RAW_DATA_DIRECTORY}/ontime.csv'\n",
    "EXTRACTED_ONTIME_FILE=f'{INTERIM_DATA_DIRECTORY}/ontime.csv'\n",
    "COMBINED_DATAFILE=f'{PROCESSED_DATA_DIRECTORY}/combined_data.csv'\n",
    "ENCODED_DATAFILE=f'{PROCESSED_DATA_DIRECTORY}/all_encoded.csv'\n",
    "TRAINING_DATAFILE=f'{PROCESSED_DATA_DIRECTORY}/train_encoded.csv'\n",
    "TESTING_DATAFILE=f'{PROCESSED_DATA_DIRECTORY}/test_encoded.csv'\n",
    "MODEL=f'{MODEL_DIRECTORY}/model.pkl'\n",
    "\n",
    "# Resource URLs\n",
    "INCIDENT_DATA_URL='https://data.ntsb.gov/avdata/FileDirectory/DownloadFile?fileID=C%3A%5Cavdata%5Cavall.zip'\n",
    "ONTIME_DATA_URL='https://www.bts.gov/browse-statistical-products-and-data/bts-publications/airline-service-quality-performance-234-time'\n",
    "ONTIME_DATA_LINK_PREFIX='https://www.bts.dot.gov'\n",
    "\n",
    "# Number of on-time performance files to download and parse\n",
    "ONTIME_FILES=3\n",
    "\n",
    "'''Functions to download extract and parse data.'''\n",
    "\n",
    "import io\n",
    "import glob\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from access_parser import AccessParser\n",
    "\n",
    "\n",
    "def download_data(url:str, raw_data_directory:str, raw_incidents_mdb_file:str) -> None:\n",
    "    '''Downloads and extract zipfile from url. Saves to raw data directory.'''\n",
    "\n",
    "    # Only download the file if we don't already have it\n",
    "    if Path(raw_incidents_mdb_file).is_file() == False:\n",
    "\n",
    "        # Get the archive from URL\n",
    "        response=requests.get(url, timeout=10)\n",
    "\n",
    "        # Extract to disk\n",
    "        archive=zipfile.ZipFile(io.BytesIO(response.content))\n",
    "        archive.extractall(raw_data_directory)\n",
    "\n",
    "\n",
    "def parse_mdb(raw_incidents_mdb_file:str, raw_incidents_csv_file:str) -> None:\n",
    "    '''Parses MDB file and saves aircraft table to csv.'''\n",
    "\n",
    "    # Only parse the file if we don't already have the result\n",
    "    if Path(raw_incidents_csv_file).is_file() == False:\n",
    "\n",
    "        # Load database and extract aircraft table\n",
    "        db=AccessParser(raw_incidents_mdb_file)\n",
    "        table=db.parse_table('aircraft')\n",
    "\n",
    "        # Convert to dataframe and save as csv\n",
    "        table_df=pd.DataFrame.from_dict(table)\n",
    "        table_df.to_csv(raw_incidents_csv_file, index=False)\n",
    "\n",
    "\n",
    "def get_ontime_links(url:str) -> list:\n",
    "    '''Uses requests and beautifulsoup to parse download links for on-time\n",
    "    performance from bts.gov site'''\n",
    "\n",
    "    # Get download links page as HTML string\n",
    "    response=requests.get(url)\n",
    "    html_content=response.text\n",
    "\n",
    "    # Convert to BeautifulSoup object and get all links from page by taking only <a> tags.\n",
    "    soup=BeautifulSoup(html_content, 'html.parser', parse_only=SoupStrainer('a'))\n",
    "    \n",
    "    # Loop on the links and collect those that point to a zip file\n",
    "    links=[]\n",
    "\n",
    "    for link in soup:\n",
    "        if link.has_attr('href'):\n",
    "            link_text=link['href']\n",
    "            if link_text.split('.')[-1] == 'zip':\n",
    "                links.append(link_text)\n",
    "\n",
    "    return links\n",
    "\n",
    "\n",
    "def download_ontime_data(links:list, ontime_data_link_prefix:str, raw_data_directory:str) -> None:\n",
    "    '''Takes list of data download urls, download the files to disk.'''\n",
    "\n",
    "    # Loop on list of links\n",
    "    for link in links:\n",
    "        \n",
    "        # Only download if we don't already have the file\n",
    "        if Path(f\"{raw_data_directory}{link.split('/')[-1]}\").is_file() == False:\n",
    "\n",
    "            # Download the zip file\n",
    "            complete_link=f'{ontime_data_link_prefix}/{link}'\n",
    "            response=requests.get(complete_link, timeout=10)\n",
    "\n",
    "            # Extract the zip file to the raw data directory\n",
    "            archive=zipfile.ZipFile(io.BytesIO(response.content))\n",
    "            archive.extractall(raw_data_directory)\n",
    "\n",
    "\n",
    "def parse_asc_datafiles(n_files:int, raw_data_directory:str, raw_ontime_csv_file:str) -> pd.DataFrame:\n",
    "    '''Reads .asc files from raw data directory, combines into\n",
    "    pandas dataframe.'''\n",
    "\n",
    "    # Get list of ASCII files from raw data directory\n",
    "    data_dfs=[]\n",
    "    asc_files=glob.glob(f'{raw_data_directory}/*.asc')\n",
    "\n",
    "    # Loop on the ASCII data files\n",
    "    for asc_file in asc_files[:n_files]:\n",
    "        print(asc_file)\n",
    "\n",
    "        # Read the file into a Pandas dataframe and collect in list\n",
    "        data_df=pd.read_table(asc_file, sep='|', low_memory=False)\n",
    "        data_dfs.append(data_df)\n",
    "\n",
    "    # Combine the list of Pandas dataframes and clean the index\n",
    "    data_df=pd.concat(data_dfs, axis=0)\n",
    "    data_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Save to CSV\n",
    "    data_df.to_csv(raw_ontime_csv_file, index=False)\n",
    "\n",
    "    return data_df\n",
    "\n",
    "# Data Acquisition \n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import functions.data_acquisition as data_funcs\n",
    "import configuration as config\n",
    "\n",
    "Path(config.RAW_DATA_DIRECTORY).mkdir(parents=True, exist_ok=True)\n",
    "Path(config.INTERIM_DATA_DIRECTORY).mkdir(parents=True, exist_ok=True)\n",
    "Path(config.PROCESSED_DATA_DIRECTORY).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "data_funcs.download_data(config.INCIDENT_DATA_URL, config.RAW_DATA_DIRECTORY, config.RAW_INCIDENTS_MDB_FILE)\n",
    "table=data_funcs.parse_mdb(config.RAW_INCIDENTS_MDB_FILE, config.RAW_INCIDENTS_CSV_FILE)\n",
    "\n",
    "links=data_funcs.get_ontime_links(config.ONTIME_DATA_URL)\n",
    "data_funcs.download_ontime_data(links[:config.ONTIME_FILES], config.ONTIME_DATA_LINK_PREFIX, config.RAW_DATA_DIRECTORY)\n",
    "\n",
    "ontime_df=data_funcs.parse_asc_datafiles(config.ONTIME_FILES, config.RAW_DATA_DIRECTORY, config.RAW_ONTIME_CSV_FILE)\n",
    "\n",
    "incidents_df=pd.read_csv(config.RAW_INCIDENTS_CSV_FILE, low_memory=False)\n",
    "incidents_df.info()\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "incidents_df.head().transpose()\n",
    "\n",
    "incident_features={\n",
    "    'dprt_time': 'departure_time',\n",
    "    'dprt_apt_id': 'origin',\n",
    "    'dest_apt_id': 'destination',\n",
    "    'regis_no': 'tail_number'\n",
    "}\n",
    "\n",
    "extracted_incident_df=incidents_df[incident_features.keys()].copy()\n",
    "extracted_incident_df=extracted_incident_df.rename(columns=incident_features)\n",
    "extracted_incident_df['incident']=[1]*len(extracted_incident_df)\n",
    "extracted_incident_df.dropna(inplace=True)\n",
    "extracted_incident_df.to_csv(config.EXTRACTED_INCIDENTS_FILE, index=False)\n",
    "extracted_incident_df.head()\n",
    "\n",
    "extracted_incident_df.info()\n",
    "ontime_df.info()\n",
    "\n",
    "ontime_features={\n",
    "    # 'carrier': 0,\n",
    "    # 'flight_number': 1,\n",
    "    'origin': 6,\n",
    "    'destination': 7,\n",
    "    # 'date': 8,\n",
    "    'departure_time': 12,\n",
    "    'tail_number': 25 \n",
    "}\n",
    "\n",
    "extracted_ontime_df=ontime_df.iloc[:,list(ontime_features.values())].copy()\n",
    "extracted_ontime_df.columns=ontime_features.keys()\n",
    "extracted_ontime_df['incident']=[0]*len(extracted_ontime_df)\n",
    "extracted_ontime_df.dropna(inplace=True)\n",
    "extracted_ontime_df.to_csv(config.EXTRACTED_ONTIME_FILE, index=False)\n",
    "extracted_ontime_df.head()\n",
    "extracted_ontime_df.info()\n",
    "\n",
    "data_df=pd.concat([extracted_ontime_df, extracted_incident_df], axis=0)\n",
    "data_df.reset_index(inplace=True, drop=True)\n",
    "data_df['origin']=data_df['origin'].astype(str)\n",
    "data_df['destination']=data_df['destination'].astype(str)\n",
    "data_df['departure_time']=data_df['departure_time'].astype(float)\n",
    "data_df['tail_number']=data_df['tail_number'].astype(str)\n",
    "data_df['incident']=data_df['incident'].astype(int)\n",
    "data_df.info()\n",
    "data_df.to_csv(config.COMBINED_DATAFILE, index=False)\n",
    "\n",
    "# data Preparation\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import configuration as config \n",
    "\n",
    "data_df=pd.read_csv(config.COMBINED_DATAFILE)\n",
    "data_df.info()\n",
    "print(data_df.head())\n",
    "airport_features=['origin','destination']\n",
    "\n",
    "print('Unique levels by feature')\n",
    "data_df[airport_features].nunique()\n",
    "print(data_df['origin'].value_counts().head(15))\n",
    "print(data_df['destination'].value_counts().head(15))\n",
    "\n",
    "# Plot origin airport level counts\n",
    "plt.figure(figsize=(15, 5))\n",
    "level_counts=data_df['origin'].value_counts().head(15).index\n",
    "sns.countplot(data=data_df, y='origin', order=level_counts)\n",
    "\n",
    "plt.title('Top 15 origin airports')\n",
    "plt.xlabel('No. of flights')\n",
    "plt.ylabel('Origin airport');\n",
    "plt.show()\n",
    "# Plot destination airport level counts\n",
    "plt.figure(figsize=(15, 5))\n",
    "level_counts=data_df['destination'].value_counts().head(15).index\n",
    "sns.countplot(data=data_df, y='destination', order=level_counts)\n",
    "\n",
    "plt.title('Top 15 destination airports')\n",
    "plt.xlabel('No. of flights')\n",
    "plt.ylabel('Destination airport');\n",
    "plt.show()\n",
    "data_df['incident'].value_counts()\n",
    "level_counts=data_df['incident'].value_counts()\n",
    "new_labels = ['Safe', 'Incident']\n",
    "plt.bar(list(range(len(level_counts))), level_counts, tick_label=level_counts.index, color=('green', 'red'))\n",
    "plt.title('Safe Vs Incident Flights')\n",
    "plt.xlabel('All Flights')\n",
    "plt.xticks(range(len(level_counts)), new_labels)\n",
    "plt.ylabel('Flights')\n",
    "plt.show()\n",
    "print(data_df.head())\n",
    "data_df['route'] = data_df['origin'] + '_' + data_df['destination']\n",
    "\n",
    "print(data_df.head())\n",
    "cyclical_encoded_data_df = data_df.copy()\n",
    "\n",
    "cyclical_encoded_data_df.head().T\n",
    "# Function to convert HHMM format to minutes since midnight\n",
    "def hhmm_to_minutes(hhmm):\n",
    "    hhmm_int = int(hhmm)\n",
    "    hours = hhmm // 100\n",
    "    minutes = hhmm % 100\n",
    "    return hours * 60 + minutes\n",
    "\n",
    "# Add minutes since midnight column\n",
    "cyclical_encoded_data_df['Time'] = cyclical_encoded_data_df['departure_time'].apply(hhmm_to_minutes)\n",
    "\n",
    "# Add formatted time label for display\n",
    "cyclical_encoded_data_df['time_label'] = cyclical_encoded_data_df['departure_time'].apply(lambda x: f\"{int(x)//100:02d}:{int(x)%100:02d}\")\n",
    "\n",
    "# Apply cyclical encoding - add two new columns\n",
    "cyclical_encoded_data_df['time_sin'] = np.sin(2 * np.pi * cyclical_encoded_data_df['Time'] / 1440)  # 1440 minutes in a day\n",
    "cyclical_encoded_data_df['time_cos'] = np.cos(2 * np.pi * cyclical_encoded_data_df['Time'] / 1440)\n",
    "\n",
    "# Display the DataFrame with the new cyclical encoding columns\n",
    "print(\"DataFrame with cyclical time encoding:\")\n",
    "print(cyclical_encoded_data_df.head(3).round(4).T)\n",
    "\n",
    "# Filter the DataFrame to only include the first 10K flights\n",
    "filtered_data = cyclical_encoded_data_df.head(10000)\n",
    "\n",
    "# Visualization of cyclical encoding\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot sine and cosine values\n",
    "plt.scatter(filtered_data['Time'], filtered_data['time_sin'], label='Sine Encoding', marker='o')\n",
    "plt.scatter(filtered_data['Time'], filtered_data['time_cos'], label='Cosine Encoding', marker='o')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.title(\"Visualization of Cyclical Encoding for Departure Times (First 10K Rows)\")\n",
    "plt.xlabel(\"Minutes Since Midnight\")\n",
    "plt.ylabel(\"Encoded Values\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "# Filter the DataFrame to only include the first 1000 flights\n",
    "filtered_data = cyclical_encoded_data_df.head(1000)\n",
    "\n",
    "# Circular Visualization\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Plot the circular points\n",
    "plt.scatter(filtered_data['time_cos'], filtered_data['time_sin'], c=filtered_data['Time'], cmap='viridis', s=50)\n",
    "\n",
    "# # Add labels for each time point\n",
    "# for i in range(filtered_data.shape[0]):\n",
    "#     plt.text(filtered_data['time_cos'].iloc[i] * 1.2,  # Adjust position slightly outside the circle\n",
    "#              filtered_data['time_sin'].iloc[i] * 1.2,\n",
    "#              filtered_data['time_label'].iloc[i],\n",
    "#              fontsize=9, ha='center', va='center')\n",
    "\n",
    "# Plot a circle for reference\n",
    "circle = plt.Circle((0, 0), 1, color='black', fill=False, linestyle='--', linewidth=0.8)\n",
    "plt.gca().add_artist(circle)\n",
    "\n",
    "# Set aspect ratio and labels\n",
    "plt.gca().set_aspect('equal', adjustable='datalim')\n",
    "plt.title(\"Circular Visualization of Cyclical Encoded Time\")\n",
    "plt.xlabel(\"Cosine Component\")\n",
    "plt.ylabel(\"Sine Component\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.colorbar(label=\"Minutes Since Midnight\")\n",
    "plt.show()\n",
    "cyclical_encoded_data_df.head(5)\n",
    "cyclical_encoded_data_df.drop(columns=['origin','destination','departure_time','tail_number','Time', 'time_label'], inplace=True)\n",
    "\n",
    "cyclical_encoded_data_df.head(5)\n",
    "\n",
    "# Frequency encoding for 'route'\n",
    "route_frequency = cyclical_encoded_data_df['route'].value_counts()\n",
    "cyclical_encoded_data_df['route_encoded'] = cyclical_encoded_data_df['route'].map(route_frequency)\n",
    "cyclical_encoded_data_df.drop(columns=['route'], inplace=True)\n",
    "\n",
    "# Display the updated dataset\n",
    "print(\"DataFrame with frequency-encoded features:\")\n",
    "print(cyclical_encoded_data_df.head(5).T)\n",
    "\n",
    "data_df=cyclical_encoded_data_df.copy()\n",
    "data_df.head(5).T\n",
    "\n",
    "train_df, test_df=train_test_split(\n",
    "    data_df,\n",
    "    test_size=0.25, \n",
    "    random_state=315\n",
    ")\n",
    "\n",
    "train_df.head(5).T\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "train_df.head(5).T\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "test_df.head(5).T\n",
    "\n",
    "Path(config.PROCESSED_DATA_DIRECTORY).mkdir(exist_ok=True)\n",
    "\n",
    "data_df.to_csv(config.ENCODED_DATAFILE)\n",
    "train_df.to_csv(config.TRAINING_DATAFILE)\n",
    "test_df.to_csv(config.TESTING_DATAFILE)\n",
    "\n",
    "# Model Training\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, RandomizedSearchCV, HalvingGridSearchCV\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "\n",
    "import configuration as config\n",
    "\n",
    "train_df=pd.read_csv(config.TRAINING_DATAFILE)\n",
    "test_df=pd.read_csv(config.TESTING_DATAFILE)\n",
    "\n",
    "def cross_val_boosting_model(training_data: pd.DataFrame, testing_data: pd.DataFrame, target_variable: str, model):\n",
    "    X_train = training_data.drop(columns=target_variable)\n",
    "    y_train = training_data[target_variable]\n",
    "    X_test = testing_data.drop(columns=target_variable)\n",
    "    y_test = testing_data[target_variable]\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    print(scores)\n",
    "    return X_train, y_train, y_test, y_pred\n",
    "\n",
    "hist_boost_model = HistGradientBoostingClassifier(random_state=42)\n",
    "X_train, y_train, y_test, y_pred = cross_val_boosting_model(train_df, test_df, 'incident', hist_boost_model)\n",
    "\n",
    "# Plot a confusion matrix to evaluate the model's performance on unseen data\n",
    "def confusion_plot(y_test, y_pred, model):\n",
    "    acc = accuracy_score(y_test, y_pred)*100\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "    fig, ax = plt.subplots(figsize=(8,6), dpi=100)\n",
    "    display = ConfusionMatrixDisplay(conf_matrix, display_labels=model.classes_)\n",
    "    ax.set(title=f'Confusion Matrix for the Diabetes Detection Model with {acc:.2f}% overall accuracy')\n",
    "    display.plot(ax=ax, values_format='.2%');\n",
    "\n",
    "confusion_plot(y_test, y_pred, hist_boost_model)\n",
    "# Your code here... Use one of the sklearn hyperparameter optimization functions to optimize the model\n",
    "def get_best_params(model, optimizer):\n",
    "    hyper_params = {\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "        'max_iter': [100, 200, 500, 1000],\n",
    "        'max_leaf_nodes': [15, 31, 63, 127],\n",
    "        'l2_regularization': [0.0, 0.1, 1.0, 10.0],\n",
    "    }\n",
    "\n",
    "    grid = optimizer(model, hyper_params, scoring='balanced_accuracy', cv=5, n_jobs=-1)\n",
    "    return grid\n",
    "\n",
    "grid = get_best_params(hist_boost_model, HalvingGridSearchCV)\n",
    "\n",
    "run_grid = True\n",
    "\n",
    "if run_grid:\n",
    "    def warn(*args, **kwargs):\n",
    "        pass\n",
    "    import warnings\n",
    "    warnings.warn = warn\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Best hyperparameters: {grid.best_params_}\")\n",
    "\n",
    "optimized_hist_boost_model = HistGradientBoostingClassifier(l2_regularization= 0.1, learning_rate= 0.01, max_iter= 1000, max_leaf_nodes= 15, random_state=42)\n",
    "\n",
    "X_train, y_train, y_test, y_pred = cross_val_boosting_model(train_df, test_df, 'incident', optimized_hist_boost_model) \n",
    "\n",
    "def get_predictions(training_data: pd.DataFrame, testing_data: pd.DataFrame, target_variable: str, model):\n",
    "    X_train = training_data.drop(columns=target_variable)\n",
    "    y_train = training_data[target_variable]\n",
    "    X_test = testing_data.drop(columns=target_variable)\n",
    "    y_test = testing_data[target_variable]\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(y_pred) \n",
    "    return y_pred, y_test\n",
    "\n",
    "y_pred, y_test = get_predictions(train_df, test_df, 'incident', optimized_hist_boost_model)\n",
    "\n",
    "confusion_plot(y_test, y_pred, optimized_hist_boost_model)\n",
    "\n",
    "def probabilities_plot(model, testing_data):\n",
    "    y_true = testing_data['incident']\n",
    "    X_test = testing_data.drop(columns=['incident'])\n",
    "    prob_incident = model.predict_proba(X_test)[:, 1]\n",
    "    plot_df = pd.DataFrame({\n",
    "        'probability': prob_incident,\n",
    "        'actual': y_true.map({0: 'Non-Incident', 1: 'Incident'})\n",
    "    })\n",
    "\n",
    "    # Plot the distributions\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data=plot_df, x='probability', hue='actual', bins=25, kde=True, stat='density', common_norm=False)\n",
    "    plt.title('Predicted Probabilities by Actual Outcome')\n",
    "    plt.xlabel('Predicted Probability of Incident')\n",
    "    plt.ylabel('Density')\n",
    "    plt.show()\n",
    "\n",
    "probabilities_plot(optimized_hist_boost_model, test_df)\n",
    "\n",
    "Path(config.MODEL_DIRECTORY).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(config.MODEL, 'wb') as output_file:\n",
    "    pickle.dump(hist_boost_model, output_file)\n",
    "    pickle.dump(optimized_hist_boost_model, output_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
